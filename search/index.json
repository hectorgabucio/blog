[{"content":"Intro Grab a coffee and join me on this journey through the challenges of implementing gRPC keepalive in a Go service behind an Nginx proxy. I\u0026rsquo;ll share my experience, the problems I encountered, and how I found a solution that actually works in production.\nThe Challenge In a recent project, I faced an interesting challenge: implementing a gRPC server stream that needed to maintain a healthy connection. This was particularly important because our client was a mobile application that could experience poor signal quality, intermittent data loss, and other connectivity issues. The natural approach would be to use gRPC\u0026rsquo;s built-in keepalive mechanism, but there was a catch - we were using Nginx as a proxy.\nUnderstanding gRPC Keepalive Before diving into the problem, let\u0026rsquo;s understand how gRPC keepalive works. The official documentation can be found in the gRPC keepalive guide.\nTCP keepalive is a well-known method of maintaining connections and detecting broken connections. When TCP keepalive is enabled, either side of the connection can send redundant packets. Once ACKed by the other side, the connection will be considered as good. If no ACK is received after repeated attempts, the connection is deemed broken.\nUnlike TCP keepalive, gRPC uses HTTP/2 which provides a mandatory PING frame (specified in RFC 7540) that can be used to:\nEstimate round-trip time Measure bandwidth-delay product Test the connection health The interval and retry mechanisms in TCP keepalive don\u0026rsquo;t quite apply to HTTP/2 PING frames because the transport is reliable. Instead, they\u0026rsquo;re replaced with a timeout value (equivalent to interval * retry) in gRPC\u0026rsquo;s PING-based keepalive implementation.\nHands-on with gRPC Keepalive Let\u0026rsquo;s explore how gRPC keepalive works in practice using a simple example. First, we\u0026rsquo;ll enable keepalive on the client side:\n1 2 3 4 5 6 7 8 private val channel = ManagedChannelBuilder .forAddress(SERVER_IP, SERVER_PORT) .usePlaintext() // Send ping frames every 10 seconds .keepAliveTime(10, TimeUnit.SECONDS) // Server should respond to ping in 15 seconds max .keepAliveTimeout(15, TimeUnit.SECONDS) .build() To see the HTTP/2 frames in action, we can run our server with debug logging enabled:\n1 env GODEBUG=http2debug=2 go run main.go When we connect our client and start the stream, we\u0026rsquo;ll see the following in the logs:\n1 2 3 4 5 6 7 8 9 10 11 2025/05/09 19:06:53 http2: Framer 0x1400018a540: read PING len=8 ping=\u0026#34;\\xa1\\x97GVkb\\xb4|\u0026#34; 2025/05/09 19:06:53 http2: Framer 0x1400018a540: wrote PING flags=ACK len=8 ping=\u0026#34;\\xa1\\x97GVkb\\xb4|\u0026#34; 2025/05/09 19:07:03 http2: Framer 0x1400018a540: read PING len=8 ping=\u0026#34;\\xd9L\\xad\u0026gt;\u0026amp;\\b\\xb8\\x86\u0026#34; 2025/05/09 19:07:03 http2: Framer 0x1400018a540: wrote PING flags=ACK len=8 ping=\u0026#34;\\xd9L\\xad\u0026gt;\u0026amp;\\b\\xb8\\x86\u0026#34; 2025/05/09 19:07:13 http2: Framer 0x1400018a540: read PING len=8 ping=\u0026#34;\\xd84|3G6|\\x1a\u0026#34; 2025/05/09 19:07:13 http2: Framer 0x1400018a540: wrote PING flags=ACK len=8 ping=\u0026#34;\\xd84|3G6|\\x1a\u0026#34; 2025/05/09 19:07:23 http2: Framer 0x1400018a540: read PING len=8 ping=\u0026#34;\\xd4\\xdeڬ\\x858\\x89\\x82\u0026#34; 2025/05/09 19:07:23 http2: Framer 0x1400018a540: wrote PING flags=ACK len=8 ping=\u0026#34;\\xd4\\xdeڬ\\x858\\x89\\x82\u0026#34; 2025/05/09 19:07:23 http2: Framer 0x1400018a540: wrote GOAWAY len=22 LastStreamID=3 ErrCode=ENHANCE_YOUR_CALM Debug=\u0026#34;too_many_pings\u0026#34; 2025/05/09 19:07:24 Context cancelled 2025/05/09 19:07:24 Stream ended As we can see, the client is sending ping frames every 10 seconds (note: this is the minimum allowed for the Java client). Even though we haven\u0026rsquo;t enabled keepalive on our server, it\u0026rsquo;s responding with PING frames with the ACK flag. However, after a few pings, it sends a GOAWAY frame with the code ENHANCE_YOUR_CALM. This happens because the server isn\u0026rsquo;t configured to accept ping frames, so it terminates the connection.\nLet\u0026rsquo;s enable keepalive on the server side as well:\n1 2 3 4 s := grpc.NewServer(grpc.KeepaliveParams(keepalive.ServerParameters{ Time: 10 * time.Second, Timeout: 15 * time.Second, })) Now the logs show bidirectional ping communication:\n1 2 3 4 5 6 2025/05/09 19:24:30 http2: Framer 0x14000200540: wrote PING len=8 ping=\u0026#34;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#34; 2025/05/09 19:24:30 http2: Framer 0x14000200540: read PING flags=ACK len=8 ping=\u0026#34;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#34; 2025/05/09 19:24:40 http2: Framer 0x14000200540: wrote PING len=8 ping=\u0026#34;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#34; 2025/05/09 19:24:40 http2: Framer 0x14000200540: read PING len=8 ping=\u0026#34;\\xa6\\x9c#UD[\\x83\\xd0\u0026#34; 2025/05/09 19:24:40 http2: Framer 0x14000200540: wrote PING flags=ACK len=8 ping=\u0026#34;\\xa6\\x9c#UD[\\x83\\xd0\u0026#34; 2025/05/09 19:24:40 http2: Framer 0x14000200540: read PING flags=ACK len=8 ping=\u0026#34;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#34; We can see both \u0026ldquo;wrote PING\u0026rdquo;, \u0026ldquo;wrote PING ACK\u0026rdquo;, \u0026ldquo;read PING\u0026rdquo;, and \u0026ldquo;read PING ACK\u0026rdquo; messages, indicating that both sides are actively sending pings to each other. This is exactly what we want!\nTesting Connection Loss Let\u0026rsquo;s see what happens when the Android app loses connection. We can simulate this by enabling airplane mode:\n1 2 3 4 5 6 7 8 9 2025/05/09 19:28:07 Received request: Hello Server! 2025/05/09 19:28:07 http2: Framer 0x14000200540: wrote PING len=8 ping=\u0026#34;\\x02\\x04\\x10\\x10\\t\\x0e\\a\\a\u0026#34; 2025/05/09 19:28:07 http2: Framer 0x14000200540: read PING flags=ACK len=8 ping=\u0026#34;\\x02\\x04\\x10\\x10\\t\\x0e\\a\\a\u0026#34; (at this point, I enabled airplane mode in the android emulator)... 2025/05/09 19:28:17 http2: Framer 0x14000200540: wrote PING len=8 ping=\u0026#34;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#34; 2025/05/09 19:28:32 Context cancelled 2025/05/09 19:28:32 Stream ended The server sends a PING frame at 19:28:17 to check the connection. After 15 seconds (our configured keepalive timeout) with no response, the server terminates the connection. This is exactly the behavior we want!\nThe Nginx Surprise After successfully testing locally, I deployed the solution to our test environment. We use Ingress Nginx as our reverse proxy for backend services, which provides load balancing, security, rate limiting, and other features. However, when testing the keepalive mechanism in this environment, something unexpected happened.\nEven with airplane mode enabled on the phone, the server was still receiving ACK responses to its PING frames. This was confusing because we knew the client was disconnected, but the server thought the connection was still alive.\nReproducing the Issue To better understand the problem, I used my toy project to reproduce the issue. I added an nginx proxy between the Android client and the server by running a Docker container with nginx, configured to forward traffic to my Go service.\nLet\u0026rsquo;s start by running our Docker Compose setup:\n1 docker-compose up --build First Weird Behavior: Missing Client Pings We immediately notice something strange - the server isn\u0026rsquo;t receiving the client\u0026rsquo;s ping frames:\n1 2 3 4 5 grpc-server-1 | 2025/05/09 17:51:17 Received request: Hello Server! grpc-server-1 | 2025/05/09 17:51:27 http2: Framer 0x40000e4540: wrote PING len=8 ping=\u0026#34;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#34; grpc-server-1 | 2025/05/09 17:51:27 http2: Framer 0x40000e4540: read PING flags=ACK len=8 ping=\u0026#34;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#34; grpc-server-1 | 2025/05/09 17:51:37 http2: Framer 0x40000e4540: wrote PING len=8 ping=\u0026#34;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#34; grpc-server-1 | 2025/05/09 17:51:37 http2: Framer 0x40000e4540: read PING flags=ACK len=8 ping=\u0026#34;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#34; Notice how we only see PINGs that the server is writing, and someone is sending ACKs. But who?\nThe Mystery Deepens Let\u0026rsquo;s try enabling airplane mode on the phone. Surely we shouldn\u0026rsquo;t see any PING ACKs now, right?\n1 2 3 4 5 6 7 8 grpc-server-1 | 2025/05/09 17:53:50 Received request: Hello Server! (I enabled airplane mode here) grpc-server-1 | 2025/05/09 17:54:00 http2: Framer 0x40000e4540: wrote PING len=8 ping=\u0026#34;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#34; grpc-server-1 | 2025/05/09 17:54:00 http2: Framer 0x40000e4540: read PING flags=ACK len=8 ping=\u0026#34;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#34; grpc-server-1 | 2025/05/09 17:54:10 http2: Framer 0x40000e4540: wrote PING len=8 ping=\u0026#34;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#34; grpc-server-1 | 2025/05/09 17:54:10 http2: Framer 0x40000e4540: read PING flags=ACK len=8 ping=\u0026#34;\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\u0026#34; Who is responding to the pings when the client has airplane mode enabled?\nThe Investigation After some research, I found a GitHub issue where someone reported the same problem:\n\u0026ldquo;Currently my grpc server is doing keep alive ping each 10 seconds and ngnix proxy is doing ack of the ping but ngnix itself is not pinging client.\u0026rdquo;\nHowever, there wasn\u0026rsquo;t a clear solution in that issue. Then I discovered an even older issue in the nginx issue tracker, closed as \u0026ldquo;WONT FIX\u0026rdquo; from 5 years ago.\nThe issue description was particularly relevant:\n\u0026ldquo;gRPC has multiple options for keepalive, which are especially relevant for streaming messages:\nhttps://github.com/grpc/grpc/blob/master/doc/keepalive.md\nWith these you are able to track disappearing clients and narrow connection latency. gRPC uses HTTP/2 ping messages for keepalives. When proxying through nginx, nginx does not passthrough ping messages to the client.\nWhen using nginx with gRPC you currently have to implement a form of keepalive yourself. Send and receive timeouts from nginx are not helpful, as we use gRPC with long-lived streaming connections.\nPossible solutions:\nAdd an option to passthrough HTTP/2 ping messages to the gRPC backend: grpc_ping_passthrough yes/no. Add ability to send keepalive pings from nginx. In this case nginx would need additional options and nginx would have to terminate the connection if there is no response to the ping messages, similar to what gRPC does.\u0026rdquo; The NGINX Team\u0026rsquo;s Response The NGINX team\u0026rsquo;s response was clear. They explained that using HTTP/2 pings to check client connectivity doesn\u0026rsquo;t work well with NGINX as a proxy because:\nA single connection to the backend may serve multiple clients (especially with HTTP/2 multiplexing) There\u0026rsquo;s no persistent connection between the client and the backend unless there\u0026rsquo;s an active request Multiple simultaneous client requests mean multiple client-server connections Their conclusion was straightforward: HTTP/2 pings only help verify direct (peer-to-peer) connections, not end-to-end connectivity through a proxy.\nThe TCP Keepalive Alternative As a possible solution, they mentioned using TCP keepalive. However, I rejected this idea because TCP keepalive is notoriously difficult to maintain, with many edge cases to handle and debug. For a great example of these challenges, you can read this insightful post from Cloudflare.\nThe Problem The standard gRPC keepalive mechanism relies on HTTP/2 ping frames to maintain connection health. However, when using Nginx as a proxy, these ping frames don\u0026rsquo;t work as expected. Nginx acknowledges (ACKs) these ping frames instead of forwarding them, effectively breaking the keepalive mechanism. This creates a situation where:\nThe client sends HTTP/2 ping frames Nginx receives and acknowledges these pings The actual gRPC server never sees these pings The connection health check fails Since the gRPC keepalive can be enabled on the server side as well, it creates an opposite situation:\nThe client connection is lost The server sends keepalive pings to the (supposedly) client Nginx acknowledges these pings The server thinks the connection is still alive and keeps it open The Failed Partial Workaround To be completely honest, my first thought was to get rid of Nginx. But I can\u0026rsquo;t just remove the reverse proxy. Maybe some other proxy would behave differently and forward PING frames?\nBut nope, I tried Envoy proxy and it behaves the same way: it just ACKs the PING frames.\nThe Server-Side Ping Attempt As a reminder, my stream is unidirectional - the server notifies the client about some messages, but the client can\u0026rsquo;t send any data (apart from the first request to establish the connection). So I had this great idea: The server should send frequent gRPC messages that act as a ping, and if it fails to send many of those messages, then I should be able to detect it and terminate the connection. That sounds easy.\nBut, after trying it\u0026hellip; another dead end. Even with the client on airplane mode, my server was happily sending gRPC messages to the stream, and nothing happened - the client obviously didn\u0026rsquo;t receive any message, but the server wouldn\u0026rsquo;t complain. It was almost like someone was queuing those messages in a buffer and sending them when the client is available\u0026hellip;\nThe Nginx Buffering Discovery That was my theory, and I was correct. It seems that Nginx (as a good citizen) buffers the messages and forwards them when it can (documentation). I thought about disabling this buffering, since it is possible to do so, but I discarded that idea - The server has many gRPC endpoints apart from this one, and I don\u0026rsquo;t want to lose the buffering benefits just for this use case. There had to be another way\u0026hellip;\nThe Successful Workaround So, this is what finally worked 🎊: a mechanism that relies on the client sending pings, and the server responding to these pings, all using gRPC messages. This approach:\nUses standard gRPC messages instead of HTTP/2 ping frames Can be properly proxied by Nginx Maintains (almost) the same functionality as the built-in keepalive mechanism Implementation Details I\u0026rsquo;ve created a demonstration repository that shows how to implement this custom keepalive mechanism: nginx-hates-grpc-keepalive.\nThis work-around introduced a breaking change in my original stream: I can\u0026rsquo;t just rely on a unidirectional server-stream. I actually need a bidirectional stream, where both client and server can send and receive ping requests.\nThis is the proto definition of the contract:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 service StreamService { // Bidirectional streaming RPC rpc Stream (stream StreamMessage) returns (stream StreamMessage) {} } message StreamMessage { oneof content { StreamRequest request = 1; StreamResponse response = 2; Ping ping = 3; Pong pong = 4; } } message StreamRequest { string query = 1; } message StreamResponse { string message = 1; int32 index = 2; } message Ping { int64 timestamp = 1; } message Pong { int64 original_timestamp = 1; int64 server_timestamp = 2; } Please note the Ping and Pong messages: those are part of our home-made keepalive system.\nThe custom ping mechanism works as follows:\nClient-side behavior:\nSends frequent ping messages to the server (StreamMessage with content Ping) Expects a ping reply from the server (StreamMessage with content Pong) Terminates the connection if no reply is received within a reasonable timeframe Server-side behavior:\nMonitors incoming ping messages from the client (StreamMessage with content Ping) and replies (StreamMessage with content Pong) Terminates the connection if no pings are received within the expected interval This bidirectional ping mechanism ensures both sides can detect connection issues and take appropriate action.\nIt is really simple to maintain and it works great - Nginx will forward these messages since they\u0026rsquo;re just gRPC messages. However, there are two important considerations to keep in mind:\nPing Frequency Finding the right balance for ping frequency is crucial:\nToo frequent pings: Unnecessary network overhead and server load Too infrequent pings: Delayed detection of connection issues Recommended: Start with 10-second intervals and adjust based on your needs Security Considerations Even with authentication in place, you need to protect against potential abuse:\nRate Limiting\nImplement a rate limiter per connection Example: Limit to 10 pings per 10-second window Terminate connections that exceed the limit This prevents DoS attacks from authenticated clients Why It\u0026rsquo;s Necessary\nAuthentication only protects against unauthorized access Once authenticated, clients can still send excessive pings Without rate limiting, a single client could overwhelm your server This is especially important for public-facing services Conclusion This journey through implementing gRPC keepalive with Nginx has taught me several important lessons:\nHTTP/2 Pings Don\u0026rsquo;t Work with Proxies\nNginx and Envoy both ACK HTTP/2 ping frames instead of forwarding them This breaks the standard gRPC keepalive mechanism The issue is well-documented but marked as \u0026ldquo;WONT FIX\u0026rdquo; by the Nginx team Server-Side Pings Aren\u0026rsquo;t Reliable\nNginx\u0026rsquo;s buffering behavior makes server-side ping detection unreliable Messages get queued even when the client is disconnected Disabling buffering isn\u0026rsquo;t a viable solution for most setups The Solution: Client-Initiated Pings\nUsing regular gRPC messages for keepalive works reliably The client sends pings, and the server responds Nginx properly forwards these messages We get the same functionality as built-in keepalive Important Considerations\nNeed to implement proper rate limiting Must balance ping frequency Should monitor connection health Requires bidirectional streams While not ideal, this workaround provides a reliable solution for maintaining healthy gRPC connections through Nginx proxies. It\u0026rsquo;s a good example of how sometimes we need to think outside the box when working with complex infrastructure setups, especially when dealing with mobile clients that may have unreliable connections.\nFor a complete implementation example, check out the nginx-hates-grpc-keepalive repository. Feel free to play with it - enable gRPC pings, try it with the Nginx or Envoy proxy, etc. 🚀\nThoughts While gRPC\u0026rsquo;s built-in keepalive mechanism works perfectly for direct client-server communication, this scenario is rare in production environments. In reality, 99% of deployments use load balancers, proxies, and other infrastructure components.\nThe proxy teams\u0026rsquo; decision to respond to HTTP/2 PING frames with ACKs makes perfect sense from their perspective. However, this creates a challenge for gRPC\u0026rsquo;s keepalive mechanism, which relies on these standard HTTP/2 frames.\nI believe gRPC should reconsider its keepalive implementation. Instead of relying on HTTP/2 standard frames, it could use gRPC standard messages for health checks. This would make it much easier to implement reliable connection monitoring in real-world scenarios, especially for:\nLong-lasting stream connections Mobile client applications Real-time data updates (similar to WebSocket functionality) Production environments with complex infrastructure In my case, this would have simplified the implementation of real-time notifications for mobile clients about relevant data changes. The current workaround works, but a native solution would be more elegant and maintainable.\n","date":"2025-05-09T00:00:00Z","permalink":"https://blog.hectorgabucio.com/p/implementing-grpc-keepalive-with-a-go-server-behind-a-nginx-proxy/","title":"Implementing gRPC Keepalive with a Go server behind a nginx proxy"}]